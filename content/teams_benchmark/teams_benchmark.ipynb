{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport pandas as pd",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "teams_data = pd.read_csv(\"2023-07-26_weaver3_teams_bench_np1.csv\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "teams_data.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data_by_mode = {}",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "modenames = [\"nview\", \"rview\", \"uview\"]\nmodenums = [0,1,2]\nteam_sizes = [4,8,16,32,64,128,256,512,1024]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# slice into dicts by league size\nfor (modename, modenum) in zip(modenames, modenums):\n    data_by_mode[modename] = teams_data.loc[teams_data[\"mode\"] == modenum]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data_by_mode_ts = {}\nfor modename in modenames:\n    data_by_mode_ts[modename] = {}\n    for team_size in team_sizes:\n        data_by_mode_ts[modename][team_size] = data_by_mode[modename].loc[data_by_mode[modename][\"team_size\"] == team_size]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "data_fixed_ts_nview = [data_by_mode_ts[\"nview\"][ts] for ts in team_sizes]\ndata_fixed_ts_rview = [data_by_mode_ts[\"rview\"][ts] for ts in team_sizes]\ndata_fixed_ts_uview = [data_by_mode_ts[\"uview\"][ts] for ts in team_sizes]\nlabels_fixed_ts = [\"team size \"+str(ts) for ts in team_sizes]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def figure_for_attribute(dfs, labels, attribute, xaxis=\"n\", xlabel=\"problem size (array of size n)\", ylabel = None, title = None, style=\"--\", scale=\"linear\"):\n    if ylabel is None:\n        ylabel = attribute\n    \n    if title is None:\n        title = f\"update benchmark ({attribute.replace('_',' ')})\"\n    \n    if len(dfs) == 0:\n        print(\"required to have at least one line to plot\")\n    \n    maxy = max(dfs[0][f\"{attribute}-mean\"])\n    miny = min(dfs[0][f\"{attribute}-mean\"])\n    for df in dfs:\n        cur_max = max(df[f\"{attribute}-mean\"])\n        cur_min = min(df[f\"{attribute}-mean\"])\n        if cur_max > maxy:\n            maxy = cur_max\n        if cur_min < miny:\n            miny = cur_min\n    \n    if labels is None:\n        print(\"required to label each line\")\n        \n    plt.figure(figsize=[10,6])\n    for df, dflabel in zip(dfs,labels):\n        plt.plot(df[xaxis], df[f\"{attribute}-mean\"], style, label=dflabel)\n    plt.title(title)\n    plt.legend()\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.xscale(scale)\n    plt.yscale(scale)\n    if scale == \"linear\":\n        ylim_base = 0\n    else:\n        ylim_base = miny*0.95\n    plt.ylim(ylim_base,int((maxy+1)*1.05))\n    plt.grid()\n    plt.savefig(f\"{title}.png\")\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "figure_for_attribute(data_fixed_ts_nview, labels_fixed_ts, \"gups\", xaxis=\"league_size\", ylabel = \"GStreamUpdates/second\", xlabel = \"league size\", title=\"1-node normal view overhead time\", style=\"*-\", scale=\"log\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "figure_for_attribute(data_fixed_ts_rview, labels_fixed_ts, \"gups\", xaxis=\"league_size\", ylabel = \"GStreamUpdates/second\", xlabel = \"league size\", title=\"1-node remote view overhead time\", style=\"*-\", scale=\"log\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "figure_for_attribute(data_fixed_ts_uview, labels_fixed_ts, \"gups\", xaxis=\"league_size\", ylabel = \"GStreamUpdates/second\", xlabel = \"league size\", title=\"1-node unamaged view overhead time\", style=\"*-\", scale=\"log\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}